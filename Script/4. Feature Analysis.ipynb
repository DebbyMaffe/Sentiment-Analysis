{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2927836a",
   "metadata": {},
   "source": [
    "# 4. Feature Analysis\n",
    "\n",
    "In addition to the *frequency* of distribution of words in the different categories of feelings implemented in the previous sections, we now perform an in-depth ***function*** *analysis* to obtain insights and improve the performance of the classification model that will be selected to best fits the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61664274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae1b10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean Title</th>\n",
       "      <th>Clean Content</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>second time buying headphones overall value money</td>\n",
       "      <td>lost first pair replaced really like fit sound...</td>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Clean Title  \\\n",
       "0  second time buying headphones overall value money   \n",
       "\n",
       "                                       Clean Content  Rating Sentiment  Target  \n",
       "0  lost first pair replaced really like fit sound...       4  Positive       2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Pre-processed Data\n",
    "df = pd.read_csv('rws_eda.csv')\n",
    "df = df.drop(df.columns[0], axis = 1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75fe797",
   "metadata": {},
   "source": [
    "Before proceeding with the vectorization methods to extract meaningful features from text, let's ***split*** *'df'* dataset into train and test sets with a *test_size* of *0.3*, meaning that **30%** of the dataset that should be allocated to the test set. In order to shuffle the data *before* splitting and to ensure randomness, let's set the parameter 'shuffle = True':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f256ed45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Shape: (70, 5)\n",
      "Test Set Shape: (30, 5)\n"
     ]
    }
   ],
   "source": [
    "# Split Data\n",
    "train, test = train_test_split(df, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(f'Train Set Shape: {train.shape}')\n",
    "print(f'Test Set Shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2dc56",
   "metadata": {},
   "source": [
    "For the further modelling section it is necessary to apply the **label encoding** technique, converting categorical variables into numerical format. In particular, let's perform ***count***, ***frequency***, and ***mean*** encoding on the splitted datasets to gain insights into *how many* instances belong to each sentiment category, the *proportion* of instances for each sentiment class, and evaluating the *relationship* between sentiment categories and the target variable. \n",
    "\n",
    "Since by using ***mean*** encoding there's a risk of data *leakage*, to avoid overfitting and improve the generalization of the model it is necessary, in further steps, to use techniques like *cross-validation* and *smoothing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20e72d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Clean Title  \\\n",
      "0                                       tozo earbuds   \n",
      "1  good value waterproof bluetooth earbuds nice f...   \n",
      "2                                        great sound   \n",
      "3                                initial review good   \n",
      "4                  astonishingly good bass tiny buds   \n",
      "\n",
      "                                       Clean Content  Rating Sentiment  \\\n",
      "0  great buds super sound also bonus wireless cha...       5  Positive   \n",
      "1  used different lowend midrange bluetooth earbu...       5  Positive   \n",
      "2       using walking dog gym really impressed money       4  Positive   \n",
      "3  received today initial review seem really good...       4  Positive   \n",
      "4  hard believe tiny buds produce bass sounds fac...       5  Positive   \n",
      "\n",
      "   Target  Count  Freq Encoded  Mean Encoded  \n",
      "0       2     59      0.842857           2.0  \n",
      "1       2     59      0.842857           2.0  \n",
      "2       2     59      0.842857           2.0  \n",
      "3       2     59      0.842857           2.0  \n",
      "4       2     59      0.842857           2.0  \n",
      "                                         Clean Title  \\\n",
      "0                         loved much bought 2nd pair   \n",
      "1                                    resilient great   \n",
      "2                   brilliant especially bass lovers   \n",
      "3  good overall reproduction awkward remove bud c...   \n",
      "4  excellent ear buds excellent customer service ...   \n",
      "\n",
      "                                       Clean Content  Rating Sentiment  \\\n",
      "0  earbuds usei gifted pink pair years ago use ti...       5  Positive   \n",
      "1  still going strong year half constant use went...       5  Positive   \n",
      "2  excellent value money first thing notice premi...       5  Positive   \n",
      "3  decided try relatively cheap ones reading one ...       4  Positive   \n",
      "4  recently sent review one earbuds failed sound ...       5  Positive   \n",
      "\n",
      "   Target  Count  Freq Encoded  Mean Encoded  \n",
      "0       2     59      0.842857           2.0  \n",
      "1       2     59      0.842857           2.0  \n",
      "2       2     59      0.842857           2.0  \n",
      "3       2     59      0.842857           2.0  \n",
      "4       2     59      0.842857           2.0  \n"
     ]
    }
   ],
   "source": [
    "# Count Encoding\n",
    "count_encoded = train.groupby('Sentiment').size().reset_index(name = 'Count')\n",
    "train = train.merge(count_encoded, on = 'Sentiment', how = 'left')\n",
    "test = test.merge(count_encoded, on = 'Sentiment', how = 'left')\n",
    "\n",
    "# Frequency Encoding\n",
    "freq_encoded = train.groupby('Sentiment').size() / len(train)\n",
    "train['Freq Encoded'] = train['Sentiment'].map(freq_encoded)\n",
    "test['Freq Encoded'] = test['Sentiment'].map(freq_encoded)\n",
    "\n",
    "# Mean Encoding\n",
    "mean_encoded = train.groupby('Sentiment')['Target'].mean()\n",
    "train['Mean Encoded'] = train['Sentiment'].map(mean_encoded)\n",
    "test['Mean Encoded'] = test['Sentiment'].map(mean_encoded)\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54daa4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean Title</th>\n",
       "      <th>Clean Content</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Target</th>\n",
       "      <th>Count</th>\n",
       "      <th>Freq Encoded</th>\n",
       "      <th>Mean Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tozo earbuds</td>\n",
       "      <td>great buds super sound also bonus wireless cha...</td>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good value waterproof bluetooth earbuds nice f...</td>\n",
       "      <td>used different lowend midrange bluetooth earbu...</td>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great sound</td>\n",
       "      <td>using walking dog gym really impressed money</td>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Clean Title  \\\n",
       "0                                       tozo earbuds   \n",
       "1  good value waterproof bluetooth earbuds nice f...   \n",
       "2                                        great sound   \n",
       "\n",
       "                                       Clean Content  Rating Sentiment  \\\n",
       "0  great buds super sound also bonus wireless cha...       5  Positive   \n",
       "1  used different lowend midrange bluetooth earbu...       5  Positive   \n",
       "2       using walking dog gym really impressed money       4  Positive   \n",
       "\n",
       "   Target  Count  Freq Encoded  Mean Encoded  \n",
       "0       2     59      0.842857           2.0  \n",
       "1       2     59      0.842857           2.0  \n",
       "2       2     59      0.842857           2.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6bea74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with Pos Sentiment: 59\n",
      "Number of rows with Neu Sentiment: 8\n",
      "Number of rows with Neg Sentiment: 3\n"
     ]
    }
   ],
   "source": [
    "sent_2 = train['Target'].value_counts().get(2, 0)\n",
    "print(\"Number of rows with Pos Sentiment:\", sent_2)\n",
    "sent_1 = train['Target'].value_counts().get(1, 0)\n",
    "print(\"Number of rows with Neu Sentiment:\", sent_1)\n",
    "sent_0 = train['Target'].value_counts().get(0, 0)\n",
    "print(\"Number of rows with Neg Sentiment:\", sent_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a90ca",
   "metadata": {},
   "source": [
    "## 4.1. Word Vectorization\n",
    "\n",
    "**Word vectorization** is a fundamental step in *preparing* text data for machine learning: it maps words or phrases from vocabulary to a corresponding vector of real numbers which used to learn *patterns*, make *predictions*, find word *similarities*, and perform various text classification tasks.\n",
    "\n",
    "Since there are several *techniques* for vectorizing text data and capture word frequencies, let's analyze two of the most common approaches: **Bag-Of-Words** and **TF-IDF** models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75a61c",
   "metadata": {},
   "source": [
    "##### BAG OF WORDS\n",
    "\n",
    "The first step involves *splitting* a sentence or a piece of text into individual words (***token***). Let's transform the *'Clean Content'* of the training dataset into a sparse matrix of word counts, and then converting it to a dense NumPy array using **toarray()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd08873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instance for Unigrams\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Bag-Of-Words\n",
    "bow = cv.fit_transform(train['Clean Content']).toarray()\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13f4b9",
   "metadata": {},
   "source": [
    "In the resulting array, each row corresponds to a document (*review*) and each column corresponds to a *unique* word in the vocabulary. The values in the BoW representation represent the ***count*** of each word in the respective text: if the word appears in the document, the corresponding value is set to the *frequency* of that word; otherwise, it's set to *zero*.\n",
    "\n",
    "Once the text is tokenized, a **vocabulary** is created by listing all *unique* words found in the entire corpus of documents, where each unique word becomes a *dimension* in the BoW representation. \n",
    "\n",
    "Let's show the list of *vocabulary* used for the CountVectorizer during the *word-level* analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89520336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 Unique Words in the corpus are: ['14hrs', '18month', '4hrs', '58x', '6th', 'ability', 'able', 'aboutsound', 'absorbed', 'accepted']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debby\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams Vocabulary\n",
    "vocab = cv.get_feature_names()\n",
    "print(f'The first 10 Unique Words in the corpus are: {cv.get_feature_names()[:10]}')\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ce386",
   "metadata": {},
   "source": [
    "Let's inspect which features are the ***most*** *important*, and which ones are useless considering the following conditions:\n",
    "* **Value 0:** indicates that a specific word (feature) is ***not*** *present* in the corresponding document. \n",
    "* **Value 1:** represents that a particular word is present in the corresponding document exactly once, indicating the *occurrence* of that feature in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3afb333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buds</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charging</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wireless</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sound</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulled</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>put</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BoW\n",
       "bonus       1\n",
       "buds        1\n",
       "charging    1\n",
       "also        1\n",
       "great       1\n",
       "wireless    1\n",
       "sound       1\n",
       "super       1\n",
       "pulled      0\n",
       "put         0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(bow[0].T, index = cv.get_feature_names(), columns = ['BoW'])\n",
    "bow_df = bow_df.sort_values('BoW', ascending = False)\n",
    "# Top 10 Most Relevant Unigrams\n",
    "bow_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a40c1",
   "metadata": {},
   "source": [
    "What about performing a *word-level* analysis including *bigrams* and *trigrams*? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f451ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sound also</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wireless charging</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also bonus</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buds super</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great buds</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus wireless</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super sound</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>products brilliant</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product however</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promptly received</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BoW\n",
       "sound also            1\n",
       "wireless charging     1\n",
       "also bonus            1\n",
       "buds super            1\n",
       "great buds            1\n",
       "bonus wireless        1\n",
       "super sound           1\n",
       "products brilliant    0\n",
       "product however       0\n",
       "promptly received     0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instance for Bigrams\n",
    "cv_bigrams = CountVectorizer(ngram_range = (2, 2))\n",
    "# BOW for Bigrams\n",
    "bow_bigrams = cv_bigrams.fit_transform(train['Clean Content'])\n",
    "bow_df_bigrams = pd.DataFrame(bow_bigrams[0].T.toarray(), index = cv_bigrams.get_feature_names_out(), columns = ['BoW'])\n",
    "bow_df_bigrams = bow_df_bigrams.sort_values('BoW', ascending = False)\n",
    "# Top 10 Most Relevant Bigrams\n",
    "bow_df_bigrams.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9391d627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>super sound also</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sound also bonus</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus wireless charging</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great buds super</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buds super sound</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also bonus wireless</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14hrs delivery time</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase working fine</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchased previously good</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchased black headphones</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            BoW\n",
       "super sound also              1\n",
       "sound also bonus              1\n",
       "bonus wireless charging       1\n",
       "great buds super              1\n",
       "buds super sound              1\n",
       "also bonus wireless           1\n",
       "14hrs delivery time           0\n",
       "purchase working fine         0\n",
       "purchased previously good     0\n",
       "purchased black headphones    0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instance for Trigrams\n",
    "cv_trigrams = CountVectorizer(ngram_range = (3, 3))\n",
    "# BoW for Trigrams\n",
    "bow_trigrams = cv_trigrams.fit_transform(train['Clean Content'])\n",
    "bow_df_trigrams = pd.DataFrame(bow_trigrams[0].T.toarray(), index = cv_trigrams.get_feature_names_out(), columns = ['BoW'])\n",
    "bow_df_trigrams = bow_df_trigrams.sort_values('BoW', ascending = False)\n",
    "# Top 10 Most Relevant Trigrams\n",
    "bow_df_trigrams.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58feff7c",
   "metadata": {},
   "source": [
    "Let's show the list of *vocabulary* used for the corresponding CountVectorizer instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab03ef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debby\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Bigrams Vocabulary\n",
    "vocab_bi = cv_bigrams.get_feature_names() \n",
    "#print(vocab_bi) \n",
    "\n",
    "# Trigrams Vocabulary\n",
    "vocab_tr = cv_trigrams.get_feature_names()                \n",
    "#print(vocab_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36ecd252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 Unique Bigrams in the corpus are: ['14hrs delivery', '18month warranty', '4hrs continuous', '58x open', '6th choice', 'ability charge', 'ability use', 'able charge', 'able hear', 'able job']\n",
      "\n",
      "The first 10 Unique Trigrams in the corpus are: ['14hrs delivery time', '18month warranty sticker', '4hrs continuous char', '58x open back', '6th choice buds', 'ability charge charging', 'ability use one', 'able charge die', 'able hear conversation', 'able job even']\n"
     ]
    }
   ],
   "source": [
    "print(f'The first 10 Unique Bigrams in the corpus are: {cv_bigrams.get_feature_names()[:10]}')\n",
    "print()\n",
    "print(f'The first 10 Unique Trigrams in the corpus are: {cv_trigrams.get_feature_names()[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292bcedb",
   "metadata": {},
   "source": [
    "Despite its semplicity, Bag-Of-Words does ***not*** consider the *order* of words in the text, treating each word independently, and does ***not*** capture the *semantic* meaning of them, considering words with similar meanings or synonyms as distinct entities. Moreover, words not present in the vocabulary are *ignored*, leading to a loss of semantic information.\n",
    "\n",
    "For these reasons, let's consider a more accurate technique: Term Frequency–Inverse Document Frequency (**TF-IDF**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9692328f",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "\n",
    "*Term Frequency-Inverse Document Frequency* (**TF-IDF**) is used to *convert* a collection of textual documents into a matrix of numerical values, taking into account the *importance* of words within a text and across the entire corpus. TF-IDF assigns ***higher*** *weights* to words that are ***rare*** across the entire dataset but *relevant* in individual texts. It is an important tools to learn sentiment patterns by identifying words that might carry significant sentiment-related information. In particular:\n",
    "\n",
    "* *Term Frequency* (**TF**) is nothing but the frequency of word in a document out of the total number of words in that document. It is a sort of *normalized* frequency score representing how *frequent* a word is. \n",
    "\n",
    "* *Document Frequency* (**DF**) is the ratio between the number of documents containing a word (W) and the total number documents in the corpus. It represents the *proportion* of documents that contain a *certain* word (W).\n",
    "\n",
    "* *Inverse Document Frequency* (**IDF**) score is nothing but the logarithm applied on the reciprocal of DF: the ***more*** *common* a word is across all documents, the ***lesser*** its *relevance* is for the current text.\n",
    "\n",
    "The Term Frequency-Inverse Document Frequency score is given by $TF \\times IDF$, indicating that the *higher* the score, the more *important* that word is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc58b85",
   "metadata": {},
   "source": [
    "Let's *convert* pre-processed data into TF-IDF features and perform a *word-level* analysis starting from unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdc8b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance for Unigrams\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF\n",
    "words = tfidf.fit_transform(train['Clean Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7315556",
   "metadata": {},
   "source": [
    "Let's show the list of *vocabulary* used for the TfidfVectorizer during the *word-level* analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f484d043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 Unique Words in the corpus are: ['14hrs' '18month' '4hrs' '58x' '6th' 'ability' 'able' 'aboutsound'\n",
      " 'absorbed' 'accepted']\n"
     ]
    }
   ],
   "source": [
    "# Word-Level Vocabulary\n",
    "vocab = tfidf.get_feature_names()\n",
    "print(f'The first 10 Unique Words in the corpus are: {tfidf.get_feature_names_out()[:10]}')\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56a186",
   "metadata": {},
   "source": [
    "In order to inspect which features are the ***most*** *important*, and which ones are useless, it is necessary to *convert* the resulting sparse matrix of word counts to a dense NumPy array using **todense()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f30d28cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>0.553146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super</th>\n",
       "      <td>0.504064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>0.309240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wireless</th>\n",
       "      <td>0.294089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buds</th>\n",
       "      <td>0.280625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.274416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charging</th>\n",
       "      <td>0.257498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sound</th>\n",
       "      <td>0.193575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulled</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>put</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TF-IDF\n",
       "bonus     0.553146\n",
       "super     0.504064\n",
       "also      0.309240\n",
       "wireless  0.294089\n",
       "buds      0.280625\n",
       "great     0.274416\n",
       "charging  0.257498\n",
       "sound     0.193575\n",
       "pulled    0.000000\n",
       "put       0.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(words[0].T.todense(), index = tfidf.get_feature_names(), columns = [\"TF-IDF\"])\n",
    "tfidf_df = tfidf_df.sort_values('TF-IDF', ascending = False)\n",
    "# Top 10 Most Relevant Unigrams\n",
    "tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e0e51",
   "metadata": {},
   "source": [
    "What about performing a *word-level* analysis including *bigrams* and *trigrams*? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01da6048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sound also</th>\n",
       "      <td>0.389931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus wireless</th>\n",
       "      <td>0.389931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also bonus</th>\n",
       "      <td>0.389931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buds super</th>\n",
       "      <td>0.389931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great buds</th>\n",
       "      <td>0.389931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super sound</th>\n",
       "      <td>0.389931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wireless charging</th>\n",
       "      <td>0.296183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>products brilliant</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product however</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promptly received</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TF-IDF\n",
       "sound also          0.389931\n",
       "bonus wireless      0.389931\n",
       "also bonus          0.389931\n",
       "buds super          0.389931\n",
       "great buds          0.389931\n",
       "super sound         0.389931\n",
       "wireless charging   0.296183\n",
       "products brilliant  0.000000\n",
       "product however     0.000000\n",
       "promptly received   0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instance for Bigrams\n",
    "tfidf_bigrams = TfidfVectorizer(ngram_range = (2, 2))\n",
    "# TF-IDF for Bigrams\n",
    "words_bigrams = tfidf_bigrams.fit_transform(train['Clean Content'])\n",
    "tfidf_df_bigrams = pd.DataFrame(words_bigrams[0].T.todense(), index = tfidf_bigrams.get_feature_names_out(), columns = [\"TF-IDF\"])\n",
    "tfidf_df_bigrams = tfidf_df_bigrams.sort_values('TF-IDF', ascending = False)\n",
    "# Top 10 Most Relevant Bigrams\n",
    "tfidf_df_bigrams.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f7f9546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>super sound also</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sound also bonus</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus wireless charging</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great buds super</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buds super sound</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also bonus wireless</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14hrs delivery time</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase working fine</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchased previously good</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchased black headphones</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              TF-IDF\n",
       "super sound also            0.408248\n",
       "sound also bonus            0.408248\n",
       "bonus wireless charging     0.408248\n",
       "great buds super            0.408248\n",
       "buds super sound            0.408248\n",
       "also bonus wireless         0.408248\n",
       "14hrs delivery time         0.000000\n",
       "purchase working fine       0.000000\n",
       "purchased previously good   0.000000\n",
       "purchased black headphones  0.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instance for Trigrams\n",
    "tfidf_trigrams = TfidfVectorizer(ngram_range = (3, 3))\n",
    "# TF-IDF for Trigrams\n",
    "words_trigrams = tfidf_trigrams.fit_transform(train['Clean Content'])\n",
    "tfidf_df_trigrams = pd.DataFrame(words_trigrams[0].T.todense(), index = tfidf_trigrams.get_feature_names_out(), columns = [\"TF-IDF\"])\n",
    "tfidf_df_trigrams = tfidf_df_trigrams.sort_values('TF-IDF', ascending = False)\n",
    "# Top 10 Most Relevant Trigrams\n",
    "tfidf_df_trigrams.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc546d",
   "metadata": {},
   "source": [
    "Let's show the list of *vocabulary* used for the corresponding TfidfVectorizer instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5509f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debby\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Bigrams Vocabulary\n",
    "vocab_bi = tfidf_bigrams.get_feature_names()\n",
    "#print(vocab_bi)\n",
    "\n",
    "# Trigrams Vocabulary\n",
    "vocab_tr = tfidf_trigrams.get_feature_names()\n",
    "#print(vocab_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9401faad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 Unique Bigrams in the corpus are: ['14hrs delivery' '18month warranty' '4hrs continuous' '58x open'\n",
      " '6th choice' 'ability charge' 'ability use' 'able charge' 'able hear'\n",
      " 'able job']\n",
      "\n",
      "The first 10 Unique Trigrams in the corpus are: ['14hrs delivery time' '18month warranty sticker' '4hrs continuous char'\n",
      " '58x open back' '6th choice buds' 'ability charge charging'\n",
      " 'ability use one' 'able charge die' 'able hear conversation'\n",
      " 'able job even']\n"
     ]
    }
   ],
   "source": [
    "print(f'The first 10 Unique Bigrams in the corpus are: {tfidf_bigrams.get_feature_names_out()[:10]}')\n",
    "print( )\n",
    "print(f'The first 10 Unique Trigrams in the corpus are: {tfidf_trigrams.get_feature_names_out()[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae94053",
   "metadata": {},
   "source": [
    "To conclude the word vectorization analysis, let's *count* and visualize the total number of unique ***words*** that *appear* in the documents across the training set to identify the most *significant* features provided by BoW and TF-IDF models. Words that are relevant in both the representations of the document are features having a *BoW* value of **1** and a *TF-IDF* value **greater** than **0**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "310edbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of significant features:  8\n",
      "Significant Words included are: ['bonus', 'buds', 'charging', 'also', 'great', 'wireless', 'sound', 'super']\n"
     ]
    }
   ],
   "source": [
    "words_df = bow_df.merge(tfidf_df, left_index = True, right_index = True, suffixes = ('_BoW', '_TFIDF'))\n",
    "total_count = ((words_df['BoW'] == 1) & (words_df['TF-IDF'] > 0)).sum()\n",
    "print('Total count of significant features: ', total_count)\n",
    "words_with_value_1 = words_df.index[(words_df['BoW'] == 1) & (words_df['TF-IDF'] > 0)].tolist()\n",
    "print('Significant Words included are:', words_with_value_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037286f0",
   "metadata": {},
   "source": [
    "What about counting the total number of the most *significant* ***bigrams*** and ***trigrams***?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b644a2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of significant features:  7\n",
      "Significant Bigrams included are: ['sound also', 'wireless charging', 'also bonus', 'buds super', 'great buds', 'bonus wireless', 'super sound']\n"
     ]
    }
   ],
   "source": [
    "bigrams_df = bow_df_bigrams.merge(tfidf_df_bigrams, left_index = True, right_index = True, suffixes = ('_BoW', '_TFIDF'))\n",
    "total_count = ((bigrams_df['BoW'] == 1) & (bigrams_df['TF-IDF'] > 0)).sum()\n",
    "print('Total count of significant features: ', total_count)\n",
    "bigrams_with_value_1 = bigrams_df.index[(bigrams_df['BoW'] == 1) & (bigrams_df['TF-IDF'] > 0)].tolist()\n",
    "print('Significant Bigrams included are:', bigrams_with_value_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bceff571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of significant features:  6\n",
      "Significant Trigrams included are: ['super sound also', 'sound also bonus', 'bonus wireless charging', 'great buds super', 'buds super sound', 'also bonus wireless']\n"
     ]
    }
   ],
   "source": [
    "trigrams_df = bow_df_trigrams.merge(tfidf_df_trigrams, left_index = True, right_index = True, suffixes = ('_BoW', '_TFIDF'))\n",
    "total_count = ((trigrams_df['BoW'] == 1) & (trigrams_df['TF-IDF'] > 0)).sum()\n",
    "print('Total count of significant features: ', total_count)\n",
    "trigrams_with_value_1 = trigrams_df.index[(trigrams_df['BoW'] == 1) & (trigrams_df['TF-IDF'] > 0)].tolist()\n",
    "print('Significant Trigrams included are:', trigrams_with_value_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29843c7",
   "metadata": {},
   "source": [
    "## 4.2. Feature Selection\n",
    "\n",
    "**Feature Selection** is the process of selecting a subset of *relevant* features from the original dataset in order to improve the *performance* of a machine learning model. In text classification, feature selection involves choosing a subset of words, phrases, or other linguistic elements as input features for the classification algorithm with the purpose to retain the most *significant* features while reducing noise and improving the model's *efficiency* and *effectiveness*.\n",
    "\n",
    "To improve model's performance and computational efficiency, it could be usefull to *remove* ***Low-variance*** features reducing the complexity of the model and the risk of overfitting, leading to faster training times and more *efficient* predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c854dbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 70 entries, 0 to 69\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Clean Title    69 non-null     object\n",
      " 1   Clean Content  70 non-null     object\n",
      " 2   Rating         70 non-null     int64 \n",
      " 3   Sentiment      70 non-null     object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Training Set\n",
    "train = train.loc[:, ['Clean Title', 'Clean Content', 'Rating', 'Sentiment']]\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c0d4b",
   "metadata": {},
   "source": [
    "Before removing low-variance features, it is necessary to ***encode*** categorical columns *(object)* into numerical format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd9e4b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean Title</th>\n",
       "      <th>Clean Content</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clean Title  Clean Content  Rating  Sentiment\n",
       "0           60             20       5          2\n",
       "1           27             59       5          2\n",
       "2           39             62       4          2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to be encoded\n",
    "columns_to_encode = ['Clean Title', 'Clean Content', 'Sentiment']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for col in columns_to_encode:\n",
    "    train[col] = encoder.fit_transform(train[col])\n",
    "    \n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc26cb6",
   "metadata": {},
   "source": [
    "Let's *remove* features with *low* variance from the train dataset, considering only the features (***X***) and not the desidered output (***y***):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be4d0537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = train.drop('Rating', axis = 1)\n",
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f3d29",
   "metadata": {},
   "source": [
    "First of all, it is necessary to specify the threshold value below which features will be considered having low variance and thus will be removed from the dataset. In particular:\n",
    "* If variance threshold **= 0**, *Constant* features will be dropped.\n",
    "* If variance threshold **> 0**, *Quasi-Constant* Features will be removed.\n",
    "\n",
    "Since variance measures how *spread out* the values of a feature are around the mean, low-variance features might ***not*** carry *significant* or discriminatory information to distinguish between different categories across classification tasks. \n",
    "\n",
    "Setting the threshold to **0.25**, let's apply the **fit_transform()** method to identify and *remove* low-variance features. Once returned the transformed feature matrix, the **get_support()** method provides a boolean mask indicating which features are selected *(True)* and which are removed *(False)* based on the variance threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd882ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Low-Variance features\n",
    "var_thr = VarianceThreshold(threshold = 0.25) \n",
    "var_thr.fit_transform(X)\n",
    "# Features to keep\n",
    "var_thr.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "246b2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "Number of retained features is:  2\n",
      "Proportion of retained features is: 66.67 %\n"
     ]
    }
   ],
   "source": [
    "# Pick up Low-Variance columns\n",
    "low = [column for column in X.columns \n",
    "          if column not in X.columns[var_thr.get_support()]]\n",
    "\n",
    "for features in low:\n",
    "    print(features)\n",
    "    \n",
    "print('Number of retained features is: ', sum(var_thr.get_support()))\n",
    "print('Proportion of retained features is: {:.2f} %'.format(sum(var_thr.get_support()) / len(X.columns) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a950d2d",
   "metadata": {},
   "source": [
    "Inspecting the results, it seems that only *'Sentiment'* feature has **not**  passed the variance threshold and has **not** been selected to be retained. Since the dataset's small size compromises its representativeness of the underlying sample, I decide to ***not*** *simplify* the model training, considering that *'Clean Title'* and *'Clean Content'* carry *significant* or discriminatory information to distinguish between different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f416fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
